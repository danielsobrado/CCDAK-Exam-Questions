## Question 21

What is the default value of the `isolation.level` setting in the Kafka consumer configuration?

- A. `read_uncommitted`
- B. `read_committed`
- C. `transactional`
- D. `none`

<details>
<summary>Response:</summary> 

**Answer:** A

**Explanation:**
The default value of the `isolation.level` setting in the Kafka consumer configuration is `read_uncommitted`. This means that by default, the consumer will read all messages, including transactional messages that are not yet committed. It may consume messages from transactions that are later aborted. If you want the consumer to only read committed messages and wait for transactions to be committed before making the messages visible, you need to explicitly set the `isolation.level` to `read_committed`. The `transactional` and `none` options are not valid values for the `isolation.level` setting.


</details>

## Question 22

What happens when a consumer with `isolation.level=read_committed` encounters a message that is part of an ongoing transaction?

- A. The consumer will read the message immediately
- B. The consumer will wait until the transaction is committed before reading the message
- C. The consumer will skip the message and move on to the next one
- D. The consumer will throw an exception and stop consuming

<details>
<summary>Response:</summary> 

**Answer:** B

**Explanation:**
When a consumer with `isolation.level=read_committed` encounters a message that is part of an ongoing transaction, the consumer will wait until the transaction is committed before reading the message. The `read_committed` isolation level ensures that the consumer only reads messages that are not part of ongoing transactions and messages that are part of committed transactions. If a message belongs to a transaction that is still in progress, the consumer will wait until the transaction is committed before making the message visible to the consumer. This behavior guarantees that the consumer only sees messages that are part of successful transactions and prevents the consumer from consuming messages that may later be rolled back if the transaction is aborted.


</details>

## Question 23

What is the purpose of the `max.poll.records` setting in the Kafka consumer configuration?

- A. To specify the maximum number of records to return in a single poll
- B. To control the maximum amount of data the consumer can receive per second
- C. To set the maximum number of partitions the consumer can subscribe to
- D. To determine the maximum number of consumers allowed in a consumer group

<details>
<summary>Response:</summary> 

**Answer:** A

**Explanation:**
The `max.poll.records` setting in the Kafka consumer configuration is used to specify the maximum number of records to return in a single poll. When the consumer calls the `poll()` method to fetch records from Kafka, it will retrieve at most `max.poll.records` records. This setting allows you to control the maximum number of records that the consumer will process in each iteration. By default, `max.poll.records` is set to 500. Adjusting this value can help balance the trade-off between latency and throughput. Setting a higher value can increase throughput by allowing the consumer to process more records in each poll, but it may also increase latency if the processing of each batch takes longer.

</details>

## Question 24

How does the `max.poll.interval.ms` setting affect the behavior of a Kafka consumer?

- A. It specifies the maximum amount of time the consumer can wait before polling for new records
- B. It sets the maximum interval between two consecutive polls before the consumer is considered dead
- C. It determines the maximum time allowed for message processing before committing offsets
- D. It controls the maximum number of records the consumer can poll in a single request

<details>
<summary>Response:</summary> 

**Answer:** B


**Explanation:**
The `max.poll.interval.ms` setting in the Kafka consumer configuration specifies the maximum interval between two consecutive polls before the consumer is considered dead. If the consumer does not call the `poll()` method within this interval, the consumer will be marked as failed and removed from the consumer group. This setting is used to detect and handle consumer failures. By default, `max.poll.interval.ms` is set to 5 minutes (300000 milliseconds). If a consumer takes longer than this interval to process a batch of records, it needs to call `poll()` again within the specified interval to avoid being considered dead. Setting an appropriate value for `max.poll.interval.ms` ensures that consumers are actively participating in the consumer group and helps detect and recover from consumer failures.

</details>

## Question 25

What happens when a Kafka consumer is marked as dead due to exceeding the `max.poll.interval.ms` interval?

- A. The consumer is automatically rebalanced, and its partitions are reassigned to other consumers in the group
- B. The consumer receives an exception and must manually rejoin the consumer group
- C. The consumer's offset commits are rolled back, and it starts consuming from the beginning of the assigned partitions
- D. The consumer is permanently removed from the consumer group and cannot rejoin

<details>
<summary>Response:</summary> 

**Answer:** A

**Explanation:**
When a Kafka consumer is marked as dead due to exceeding the `max.poll.interval.ms` interval, the consumer is automatically rebalanced, and its partitions are reassigned to other consumers in the consumer group. The Kafka consumer group coordinator detects that the consumer has failed to poll within the specified interval and triggers a rebalance operation. During the rebalance, the partitions assigned to the dead consumer are revoked and redistributed among the remaining active consumers in the group. This ensures that the workload is evenly distributed and that the consumer group continues to make progress. The dead consumer is removed from the group, and it needs to rejoin the group and receive new partition assignments to start consuming again.

Certainly! Here are 3 more questions based on question 41 in the document:

</details>

## Question 26

What triggers a partition rebalance in a Kafka consumer group?

- A. Adding a new topic to the Kafka cluster
- B. Changing the replication factor of a topic
- C. Adding a new consumer to the consumer group
- D. Modifying the consumer group ID

<details>
<summary>Response:</summary> 

**Answer:** C

**Explanation:**
A partition rebalance in a Kafka consumer group is triggered when there is a change in the group membership. Specifically, adding a new consumer to the consumer group will trigger a rebalance. During a rebalance, Kafka reassigns the partitions to the consumers in the group to ensure an even distribution of work. This allows the new consumer to start consuming messages from the assigned partitions. Other events, such as removing a consumer from the group or a consumer voluntarily leaving the group, will also trigger a rebalance. However, adding a new topic to the cluster, changing the replication factor of a topic, or modifying the consumer group ID do not directly trigger a rebalance.

</details>

## Question 27

What happens to the partition assignments during a consumer group rebalance?

- A. Partitions are evenly distributed among the remaining consumers
- B. Partitions are assigned to the consumers based on the consumer group ID
- C. Partitions are randomly assigned to the consumers
- D. Partitions are assigned to the consumers based on the topic name

<details>
<summary>Response:</summary> 

**Answer:** A

**Explanation:**
During a consumer group rebalance, Kafka reassigns the partitions to the consumers in the group to ensure an even distribution of work. The partitions are evenly distributed among the remaining active consumers in the group. Kafka uses a partition assignment strategy, such as the range or round-robin strategy, to determine which consumer gets assigned which partitions. The assignment strategy aims to balance the workload and ensure that each consumer receives a fair share of the partitions. The partition assignments are not based on factors like the consumer group ID or the topic name, but rather on the number of active consumers and the available partitions.



</details>
